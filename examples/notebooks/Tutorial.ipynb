{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d049232-f4b1-473d-aac3-0b3539905b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T10:44:22.839382Z",
     "start_time": "2025-01-22T10:44:18.410638Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from terratorch import BACKBONE_REGISTRY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e53951",
   "metadata": {},
   "source": [
    "Check out the quick start in the docs for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83560911-91ed-4586-b4ec-9a3bcd80fce4",
   "metadata": {},
   "source": [
    "# Backbone factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdfa85-8e43-4db0-9ddf-cb11c5544942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T10:44:59.384413Z",
     "start_time": "2025-01-22T10:44:59.380583Z"
    }
   },
   "outputs": [],
   "source": [
    "print([model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c6071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T10:45:05.471003Z",
     "start_time": "2025-01-22T10:45:05.466191Z"
    }
   },
   "outputs": [],
   "source": [
    "\"prithvi_eo_v2_100_tl\" in BACKBONE_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db3f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T10:44:34.736849Z",
     "start_time": "2025-01-22T10:44:34.220986Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_100_tl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b937c-cbab-4e34-a113-874fa9bac983",
   "metadata": {},
   "source": [
    "## The model\n",
    "The resulting model is a torch module, but only the encoder (backbone) portion.\n",
    "\n",
    "In this case, it comes from the underlying timm registry, and is wrapped in a `TimmBackboneWrapper`.\n",
    "\n",
    "By default, the model is instantiated with the same bands it was pretrained on, with the same order.\n",
    "\n",
    "We can inspect both of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1040a09-fa6c-40e2-9a6f-8a3a60c520b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The model was pretrained on bands {model.pretrained_bands}.\\n The model is using bands {model.model_bands}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb25d1f-ad67-4b11-b6bd-fdfcd0ad32e5",
   "metadata": {},
   "source": [
    "The model output is a list with the output of each encoder stage. This may be different for each encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f19710-1bec-4cb1-a5ec-d9a51556dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = torch.zeros(1, 6, 224, 224) # batch_size, channels, height, width\n",
    "features = model(trial_data)\n",
    "for index, feature in enumerate(features):\n",
    "    print(f\"Feature index {index} has shape {feature.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b560ae-88a6-4e8a-8a9d-a88ea8c5aaef",
   "metadata": {},
   "source": [
    "## Band choice\n",
    "Sometimes you may wish to use a separate set of bands than was used in pretraining. This may be a different ordering, a subset, a superset, or a completely different set.\n",
    "\n",
    "To do this, you may specify the bands you wish to train on using a mixture of integers and members of the `HLSBands` enum.\n",
    "\n",
    "In the patch embed layer, the weights corresponding to bands that exist in the pretrained bands will be mapped to the correct order. Bands that do not exist will be randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e98bf-c748-47c6-b56f-98c96304ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datasets import HLSBands\n",
    "\n",
    "# lets get only the RGB bands, and put them in that order rather than BGR, and lets add an extra band not in HLSBands\n",
    "bands = [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE, 14]\n",
    "model = BACKBONE_REGISTRY.build( # let's use a vit model this time\n",
    "    \"prithvi_eo_v2_100_tl\", num_frames=1, pretrained=True, bands=bands\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850f30d-eebe-4da2-8bf0-f58e3fd2ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model now expects 4 channels, not 6\n",
    "trial_data = torch.zeros(1, 4, 224, 224) # batch_size, channels, height, width\n",
    "features = model(trial_data)\n",
    "for index, feature in enumerate(features):\n",
    "    print(f\"Feature index {index} has shape {feature.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961690ed-32ec-4e51-8b56-8dff89efed62",
   "metadata": {},
   "source": [
    "# Model factory\n",
    "The model factories let us create full models ready for specific tasks, including decoders and task specific heads.\n",
    "They create normal `torch.nn.Module` s that you can use anywhere in your code.\n",
    "\n",
    "Lets create a model for semantic segmentation with 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8503c7-22c5-48b6-9edc-ee80d83b2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.models import EncoderDecoderFactory\n",
    "\n",
    "model_factory = EncoderDecoderFactory()\n",
    "\n",
    "# Let's build a segmentation model\n",
    "# Parameters prefixed with backbone_ get passed to the backbone\n",
    "# Parameters prefixed with decoder_ get passed to the decoder\n",
    "# Parameters prefixed with head_ get passed to the head\n",
    "\n",
    "model = model_factory.build_model(task=\"segmentation\",\n",
    "        backbone=\"prithvi_eo_v2_100_tl\",\n",
    "        decoder=\"FCNDecoder\",\n",
    "        backbone_bands=[\n",
    "            HLSBands.BLUE,\n",
    "            HLSBands.GREEN,\n",
    "            HLSBands.RED,\n",
    "            HLSBands.NIR_NARROW,\n",
    "            HLSBands.SWIR_1,\n",
    "            HLSBands.SWIR_2,\n",
    "        ],\n",
    "        num_classes=2,\n",
    "        backbone_pretrained=True,\n",
    "        backbone_num_frames=1,\n",
    "        decoder_channels=128,\n",
    "        head_dropout=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ec4cf-0dbc-4fe4-a0fd-03973710b6fa",
   "metadata": {},
   "source": [
    "Their output is a `ModelOutput` object, including the main `output` and the output of any `auxiliary_heads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432377b-189e-4268-80ea-d9eea721123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = torch.zeros(1, 6, 224, 224) # batch_size, channels, height, width\n",
    "out = model(trial_data)\n",
    "print(out.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366995b-2e37-4155-9a26-1d0ad842a60c",
   "metadata": {},
   "source": [
    "# Datamodule\n",
    "You can create datamodules for training by creating your own subclasses of `torchgeo.datamodules.GeoDataModule` or `torchgeo.datamodules.NonGeoDataModule`.\n",
    "\n",
    "Alternatively, leverage one of our generic data modules.\n",
    "\n",
    "Datamodules package train, test and validation datasets as well as any transforms done.\n",
    "\n",
    "Any [TorchGeo](https://torchgeo.readthedocs.io/en/stable/) datamodule will also be compatible with TerraTorch.\n",
    "\n",
    "## Toy example\n",
    "\n",
    "Let's set up a toy example for burn scar segmentation.\n",
    "\n",
    "You can get the full dataset at https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars.\n",
    "\n",
    "For this example, we will use a single image which we will download from the demo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd to the notebooks dir if not there\n",
    "%cd examples/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the image\n",
    "!wget https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-Burn-scars-demo/resolve/main/subsetted_512x512_HLS.S30.T10TGS.2018285.v1.4_merged.tif\n",
    "\n",
    "input_file_name = \"subsetted_512x512_HLS.S30.T10TGS.2018285.v1.4_merged.tif\"\n",
    "label_file_name = \"subsetted_512x512_HLS.S30.T10TGS.2018285.v1.4.mask.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba69b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize the data directory\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "\n",
    "if not os.path.isdir(\"burn_scar_segmentation_toy\"):\n",
    "    os.mkdir(\"burn_scar_segmentation_toy\")\n",
    "\n",
    "    for data_dir in [\"train_images\", \"test_images\", \"val_images\"]:\n",
    "        os.mkdir(os.path.join(\"burn_scar_segmentation_toy\", data_dir))\n",
    "        shutil.copy(input_file_name, os.path.join(\"burn_scar_segmentation_toy\", data_dir, input_file_name))\n",
    "\n",
    "    for label_dir in [\"train_labels\", \"test_labels\", \"val_labels\"]:\n",
    "        os.mkdir(os.path.join(\"burn_scar_segmentation_toy\", label_dir))\n",
    "        shutil.copy(label_file_name, os.path.join(\"burn_scar_segmentation_toy\", label_dir, label_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf1522",
   "metadata": {},
   "source": [
    "## Let's visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n",
    "ax[0].imshow(rio.open_rasterio(input_file_name).sel(band=[3, 2, 1]).transpose(\"y\", \"x\", \"band\").to_numpy())\n",
    "ax[1].imshow(rio.open_rasterio(label_file_name).to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2e286-e5b2-4cf4-8aa7-44e874929117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "train_val_test = [\n",
    "    \"burn_scar_segmentation_toy/train_images\",\n",
    "    \"burn_scar_segmentation_toy/val_images\",\n",
    "    \"burn_scar_segmentation_toy/test_images\",\n",
    "]\n",
    "\n",
    "train_val_test_labels = {\n",
    "    \"train_label_data_root\": \"burn_scar_segmentation_toy/train_labels\",\n",
    "    \"val_label_data_root\": \"burn_scar_segmentation_toy/val_labels\",\n",
    "    \"test_label_data_root\": \"burn_scar_segmentation_toy/test_labels\",\n",
    "}\n",
    "\n",
    "# from https://github.com/NASA-IMPACT/hls-foundation-os/blob/main/configs/burn_scars.py\n",
    "means=[\n",
    "        0.033349706741586264,\n",
    "        0.05701185520536176,\n",
    "        0.05889748132001316,\n",
    "        0.2323245113436119,\n",
    "        0.1972854853760658,\n",
    "        0.11944914225186566,\n",
    "    ]\n",
    "stds=[\n",
    "        0.02269135568823774,\n",
    "        0.026807560223070237,\n",
    "        0.04004109844362779,\n",
    "        0.07791732423672691,\n",
    "        0.08708738838140137,\n",
    "        0.07241979477437814,\n",
    "    ]\n",
    "datamodule = GenericNonGeoSegmentationDataModule(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    train_data_root=train_val_test[0],\n",
    "    val_data_root=train_val_test[1],\n",
    "    test_data_root=train_val_test[2],\n",
    "    means=means,\n",
    "    stds=stds,\n",
    "    num_classes=2,\n",
    "    img_grep=\"*_merged.tif\", # img grep\n",
    "    label_grep=\"*.mask.tif\", # label grep\n",
    "    **train_val_test_labels,\n",
    "\n",
    "    # if transforms are defined with Albumentations, you can pass them here\n",
    "    # train_transform=train_transform,\n",
    "    # val_transform=val_transform,\n",
    "    # test_transform=test_transform,\n",
    "\n",
    "    # edit the below for your usecase\n",
    "    dataset_bands=[\n",
    "        HLSBands.BLUE,\n",
    "        HLSBands.GREEN,\n",
    "        HLSBands.RED,\n",
    "        HLSBands.NIR_NARROW,\n",
    "        HLSBands.SWIR_1,\n",
    "        HLSBands.SWIR_2,\n",
    "    ],\n",
    "    output_bands=[\n",
    "        HLSBands.BLUE,\n",
    "        HLSBands.GREEN,\n",
    "        HLSBands.RED,\n",
    "        HLSBands.NIR_NARROW,\n",
    "        HLSBands.SWIR_1,\n",
    "        HLSBands.SWIR_2,\n",
    "    ],\n",
    "    no_data_replace=0,\n",
    "    no_label_replace=-1,\n",
    ")\n",
    "# we want to access some properties of the train dataset later on, so lets call setup here\n",
    "# if not, we would not need to\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b6112-d818-48ea-86d6-8095cd3dd93b",
   "metadata": {},
   "source": [
    "# Lightning Trainers\n",
    "At the highest level of abstraction, you can operate with task specific trainers. These encapsulate the model, loss, optimizer and any training hyperparameters.\n",
    "\n",
    "They build on the model factory we introduced previously and are able to take any. To use a task with a model not supported by a currently existing model factory, simply create your own model factory!\n",
    "\n",
    "Let's create a Trainer for Semantic Segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3f466-5ee4-4897-8e1e-1cfafe6c4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from terratorch.models.model import AuxiliaryHead\n",
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "\n",
    "epochs = 1 # 1 epoch for demo\n",
    "lr = 1e-3\n",
    "model_args = {\n",
    "        \"backbone\":\"prithvi_eo_v2_100_tl\",\n",
    "        \"decoder\":\"FCNDecoder\",\n",
    "        \"num_classes\": 2,\n",
    "        \"backbone_bands\": [\n",
    "            HLSBands.RED,\n",
    "            HLSBands.GREEN,\n",
    "            HLSBands.BLUE,\n",
    "            HLSBands.NIR_NARROW,\n",
    "            HLSBands.SWIR_1,\n",
    "            HLSBands.SWIR_2,\n",
    "        ],\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\":1, # this is the default\n",
    "        \"decoder_channels\":128,\n",
    "        \"head_dropout\":0.2,\n",
    "        \"necks\": [\n",
    "            {\"name\": \"SelectIndices\", \"indices\": [-1]},\n",
    "            {\"name\": \"ReshapeTokensToImage\"}\n",
    "        ]\n",
    "}\n",
    "\n",
    "task = SemanticSegmentationTask(\n",
    "    model_args,\n",
    "    \"EncoderDecoderFactory\",\n",
    "    loss=\"ce\",\n",
    "    aux_loss={\"fcn_aux_head\": 0.4},\n",
    "    lr=lr,\n",
    "    ignore_index=-1,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_hparams={\"weight_decay\": 0.05},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6a6a7-4613-472e-9f70-fbe28e0e4540",
   "metadata": {},
   "source": [
    "Now we can use a Lightning Trainer to train this model on the datamodule we specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db99b3-72bb-46ba-b149-49493529d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, RichProgressBar\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "accelerator = \"gpu\"\n",
    "experiment = \"tutorial\"\n",
    "if not os.path.isdir(\"tutorial_experiments\"):\n",
    "    os.mkdir(\"tutorial_experiments\")\n",
    "default_root_dir = os.path.join(\"tutorial_experiments\", experiment)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=task.monitor, save_top_k=1, save_last=True)\n",
    "early_stopping_callback = EarlyStopping(monitor=task.monitor, min_delta=0.00, patience=20)\n",
    "logger = TensorBoardLogger(save_dir=default_root_dir, name=experiment)\n",
    "\n",
    "trainer = Trainer(\n",
    "    # precision=\"16-mixed\",\n",
    "    accelerator=accelerator,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        checkpoint_callback,\n",
    "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "    ],\n",
    "    logger=logger,\n",
    "    max_epochs=epochs, # train only one epoch for demo\n",
    "    default_root_dir=default_root_dir,\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=200\n",
    "\n",
    ")\n",
    "trainer.fit(model=task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977b153-e9e8-4ef0-a3ff-714ff117a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e22c1-d5f3-4910-9be7-76eea0610ec5",
   "metadata": {},
   "source": [
    "# Configs\n",
    "Alternatively, define all this in a config file and run it through the cli. This is the reccomended approach.\n",
    "\n",
    "Check out example config files under `examples/confs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba9b09-f029-4559-acea-20614be4b788",
   "metadata": {},
   "source": [
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "terra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
